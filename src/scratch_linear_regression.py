"""
IMPLEMENTACI√ìN DE REGRESI√ìN LINEAL DESDE CERO
Problemas 1 al 11 - Scratch Linear Regression
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import os

# =============================================================================
# [PROBLEMA 4] Funci√≥n MSE
# =============================================================================
def MSE(y_pred, y):
    """
    Error Cuadr√°tico Medio (Mean Squared Error)
    """
    return np.mean((y_pred - y) ** 2)

# =============================================================================
# [PROBLEMAS 1-3, 5] Clase ScratchLinearRegression
# =============================================================================
class ScratchLinearRegression():
    """
    Implementaci√≥n desde cero de Regresi√≥n Lineal
    """
    
    def __init__(self, num_iter=1000, lr=0.01, no_bias=False, verbose=False):
        self.iter = num_iter
        self.lr = lr
        self.no_bias = no_bias
        self.verbose = verbose
        self.loss = np.zeros(self.iter)
        self.val_loss = np.zeros(self.iter)
        self.coef_ = None
    
    # [PROBLEMA 1] Funci√≥n de suposici√≥n
    def _linear_hypothesis(self, X):
        """Calcula la hip√≥tesis lineal hŒ∏(x) = Œ∏·µÄx"""
        return X @ self.coef_
    
    # [PROBLEMA 2] M√©todo de descenso de gradiente
    def _gradient_descent(self, X, error):
        """Actualiza par√°metros usando descenso de gradiente"""
        m = X.shape[0]
        gradient = (X.T @ error) / m
        self.coef_ = self.coef_ - self.lr * gradient
    
    # [PROBLEMA 5] Funci√≥n objetivo
    def _objective_function(self, y, y_pred):
        """Funci√≥n de p√©rdida J(Œ∏) = 1/(2m) * Œ£(hŒ∏(x‚Å±) - y‚Å±)¬≤"""
        m = len(y)
        return np.sum((y_pred - y) ** 2) / (2 * m)
    
    def fit(self, X, y, X_val=None, y_val=None):
        """Entrena el modelo de regresi√≥n lineal"""
        # [PROBLEMA 8] Manejo del bias term
        if not self.no_bias:
            X = np.c_[np.ones(X.shape[0]), X]
            if X_val is not None:
                X_val = np.c_[np.ones(X_val.shape[0]), X_val]
        
        # Inicializar par√°metros
        n_features = X.shape[1]
        self.coef_ = np.random.randn(n_features)
        
        # Loop de entrenamiento
        for i in range(self.iter):
            # [PROBLEMA 1] Calcular predicciones
            y_pred = self._linear_hypothesis(X)
            
            # [PROBLEMA 5] Calcular p√©rdida
            self.loss[i] = self._objective_function(y, y_pred)
            
            # P√©rdida de validaci√≥n
            if X_val is not None and y_val is not None:
                y_val_pred = self.predict(X_val)
                self.val_loss[i] = self._objective_function(y_val, y_val_pred)
            
            # [PROBLEMA 2] Actualizar par√°metros
            error = y_pred - y
            self._gradient_descent(X, error)
            
            # Verbose output
            if self.verbose and i % 100 == 0:
                if X_val is not None and y_val is not None:
                    print(f"Iter {i}: Train Loss = {self.loss[i]:.6f}, Val Loss = {self.val_loss[i]:.6f}")
                else:
                    print(f"Iter {i}: Train Loss = {self.loss[i]:.6f}")
    
    # [PROBLEMA 3] Funci√≥n de predicci√≥n
    def predict(self, X):
        """Realiza predicciones con el modelo entrenado"""
        if not self.no_bias and X.shape[1] == self.coef_.shape[0] - 1:
            X = np.c_[np.ones(X.shape[0]), X]
        return self._linear_hypothesis(X)

# =============================================================================
# [PROBLEMA 7] Funci√≥n para curvas de aprendizaje
# =============================================================================
def plot_learning_curve(train_loss, val_loss=None, title="Curva de Aprendizaje", save_path=None):
    """Grafica la curva de aprendizaje"""
    plt.figure(figsize=(10, 6))
    iterations = range(len(train_loss))
    
    plt.plot(iterations, train_loss, label='P√©rdida Entrenamiento', linewidth=2)
    
    if val_loss is not None:
        plt.plot(iterations, val_loss, label='P√©rdida Validaci√≥n', linewidth=2)
    
    plt.xlabel('Iteraciones')
    plt.ylabel('P√©rdida')
    plt.title(title)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    if save_path:
        plt.savefig(save_path)
        print(f"Gr√°fico guardado: {save_path}")
    plt.close()

# =============================================================================
# [PROBLEMA 9] Funci√≥n para caracter√≠sticas polin√≥micas
# =============================================================================
def create_polynomial_features(X, degree=2):
    """Crea caracter√≠sticas polin√≥micas"""
    poly = PolynomialFeatures(degree=degree, include_bias=False)
    X_poly = poly.fit_transform(X)
    scaler = StandardScaler()
    return scaler.fit_transform(X_poly)

# =============================================================================
# [PROBLEMA 10] Derivaci√≥n de la f√≥rmula de actualizaci√≥n
# =============================================================================
def derivacion_formula_actualizacion():
    """
    Deriva matem√°ticamente la f√≥rmula de actualizaci√≥n del descenso de gradiente
    """
    print("\n" + "="*70)
    print("PROBLEMA 10: DERIVACI√ìN MATEM√ÅTICA DE LA F√ìRMULA DE ACTUALIZACI√ìN")
    print("="*70)
    
    # Explicaci√≥n paso a paso con notaci√≥n matem√°tica correcta
    derivacion = """
    OBJETIVO: Derivar la f√≥rmula de actualizaci√≥n
    
        Œ∏_j := Œ∏_j - Œ± * (1/m) * Œ£[hŒ∏(x‚Å±) - y‚Å±] * x_j‚Å±
    
    donde:
        Œ∏_j: par√°metro j-√©simo
        Œ±: tasa de aprendizaje (learning rate)
        m: n√∫mero de ejemplos de entrenamiento
        hŒ∏(x‚Å±): predicci√≥n para el ejemplo i
        y‚Å±: valor real para el ejemplo i
        x_j‚Å±: caracter√≠stica j del ejemplo i

    PASO 1: F√ìRMULA GENERAL DEL DESCENSO DE GRADIENTE
    
        Œ∏_j := Œ∏_j - Œ± * ‚àÇJ(Œ∏)/‚àÇŒ∏_j

    PASO 2: FUNCI√ìN DE P√âRDIDA J(Œ∏)

        J(Œ∏) = (1/(2m)) * Œ£[hŒ∏(x‚Å±) - y‚Å±]¬≤

    PASO 3: CALCULAR LA DERIVADA PARCIAL ‚àÇJ(Œ∏)/‚àÇŒ∏_j

        ‚àÇJ(Œ∏)/‚àÇŒ∏_j = ‚àÇ/‚àÇŒ∏_j [ (1/(2m)) * Œ£[hŒ∏(x‚Å±) - y‚Å±]¬≤ ]

    PASO 4: APLICAR REGLA DE LA CADENA

        = (1/(2m)) * Œ£ 2[hŒ∏(x‚Å±) - y‚Å±] * ‚àÇ/‚àÇŒ∏_j [hŒ∏(x‚Å±) - y‚Å±]

        = (1/m) * Œ£ [hŒ∏(x‚Å±) - y‚Å±] * ‚àÇ/‚àÇŒ∏_j hŒ∏(x‚Å±)

    PASO 5: DERIVAR LA FUNCI√ìN DE HIP√ìTESIS hŒ∏(x‚Å±)

        hŒ∏(x‚Å±) = Œ∏‚ÇÄ + Œ∏‚ÇÅx‚ÇÅ‚Å± + Œ∏‚ÇÇx‚ÇÇ‚Å± + ... + Œ∏_jx_j‚Å± + ... + Œ∏_nx_n‚Å±

        ‚àÇ/‚àÇŒ∏_j hŒ∏(x‚Å±) = ‚àÇ/‚àÇŒ∏_j [Œ∏‚ÇÄ + Œ∏‚ÇÅx‚ÇÅ‚Å± + ... + Œ∏_jx_j‚Å± + ... + Œ∏_nx_n‚Å±]
                       = x_j‚Å±

    PASO 6: SUSTITUIR EN LA DERIVADA

        ‚àÇJ(Œ∏)/‚àÇŒ∏_j = (1/m) * Œ£ [hŒ∏(x‚Å±) - y‚Å±] * x_j‚Å±

    PASO 7: SUSTITUIR EN LA F√ìRMULA DEL DESCENSO DE GRADIENTE

        Œ∏_j := Œ∏_j - Œ± * ‚àÇJ(Œ∏)/‚àÇŒ∏_j
              = Œ∏_j - Œ± * (1/m) * Œ£ [hŒ∏(x‚Å±) - y‚Å±] * x_j‚Å±

    ¬°F√ìRMULA DERIVADA!

    DEMOSTRACI√ìN EN NOTACI√ìN VECTORIAL/MATRICIAL:

    Para todos los par√°metros simult√°neamente:

        ‚àáJ(Œ∏) = (1/m) * X·µÄ(XŒ∏ - y)

        Œ∏ := Œ∏ - Œ± * ‚àáJ(Œ∏)
            = Œ∏ - Œ± * (1/m) * X·µÄ(XŒ∏ - y)

    donde:
        X: matriz de dise√±o (m √ó n+1)
        y: vector de valores reales (m √ó 1)
        Œ∏: vector de par√°metros ((n+1) √ó 1)

    VERIFICACI√ìN DE LA IMPLEMENTACI√ìN:

    En nuestro c√≥digo, en el m√©todo _gradient_descent:

        error = hŒ∏(X) - y = XŒ∏ - y
        gradient = (X.T @ error) / m = (1/m) * X·µÄ(XŒ∏ - y)
        self.coef_ = self.coef_ - self.lr * gradient

    ¬°Que es exactamente la f√≥rmula derivada!
    """
    print(derivacion)
    
    # Demostraci√≥n con ejemplo num√©rico simple
    print("\n" + "-"*50)
    print("DEMOSTRACI√ìN NUM√âRICA CON EJEMPLO SIMPLE")
    print("-"*50)
    
    # Crear ejemplo m√≠nimo
    np.random.seed(42)
    m = 5  # 5 ejemplos
    n = 2  # 2 caracter√≠sticas (m√°s bias)
    
    # Datos de ejemplo
    X = np.c_[np.ones(m), np.random.randn(m, n)]  # X con bias term
    y = np.random.randn(m)
    theta = np.random.randn(n+1)
    alpha = 0.01
    
    print(f"Matriz X (con bias):\n{X}")
    print(f"Vector y: {y}")
    print(f"Par√°metros Œ∏ iniciales: {theta}")
    print(f"Tasa de aprendizaje Œ±: {alpha}")
    
    # Calcular manualmente la derivada
    predictions = X @ theta
    error = predictions - y
    gradient_manual = (X.T @ error) / m
    
    print(f"\nC√°lculo manual del gradiente:")
    print(f"Predicciones hŒ∏(X): {predictions}")
    print(f"Error (hŒ∏(X) - y): {error}")
    print(f"Gradiente ‚àáJ(Œ∏) = (1/m) * X·µÄ(XŒ∏ - y): {gradient_manual}")
    
    # Mostrar la actualizaci√≥n
    theta_updated = theta - alpha * gradient_manual
    print(f"\nActualizaci√≥n:")
    print(f"Œ∏_nuevo = Œ∏ - Œ± * ‚àáJ(Œ∏) = {theta_updated}")
    
    # Verificar con nuestra implementaci√≥n
    model = ScratchLinearRegression(num_iter=1, lr=alpha, verbose=False)
    model.coef_ = theta.copy()
    model._gradient_descent(X, error)
    
    print(f"\nVerificaci√≥n con nuestra implementaci√≥n:")
    print(f"Œ∏ despu√©s de _gradient_descent: {model.coef_}")
    print(f"¬øCoinciden? {np.allclose(theta_updated, model.coef_)}")
    
    # Explicaci√≥n adicional
    print("\n" + "-"*50)
    print("INTERPRETACI√ìN INTUITIVA")
    print("-"*50)
    interpretacion = """
    SIGNIFICADO INTUITIVO DE LA F√ìRMULA:
    
    La actualizaci√≥n para cada par√°metro Œ∏_j es:
    
        Œ∏_j := Œ∏_j - Œ± * (1/m) * Œ£ error‚Å± * x_j‚Å±
    
    Donde:
        - error‚Å± = hŒ∏(x‚Å±) - y‚Å±: qu√© tan equivocada est√° nuestra predicci√≥n
        - x_j‚Å±: valor de la caracter√≠stica j en el ejemplo i
        - (1/m): promedio sobre todos los ejemplos
        - Œ±: tama√±o del paso (qu√© tan r√°pido aprendemos)
    
    INTERPRETACI√ìN:
    - Si el error es grande y positivo (sobrestimamos), reducimos Œ∏_j
    - Si el error es grande y negativo (subestimamos), aumentamos Œ∏_j  
    - El ajuste es proporcional al valor de la caracter√≠stica x_j‚Å±
    - Aprendemos del promedio de todos los ejemplos (no solo uno)
    
    Esto hace que el algoritmo aprenda de manera estable y converja al √≥ptimo.
    """
    print(interpretacion)

# =============================================================================
# [PROBLEMA 11] Explicaci√≥n de convexidad
# =============================================================================
def demostrar_convexidad():
    """
    Demuestra por qu√© la regresi√≥n lineal no tiene √≥ptimos locales
    mediante f√≥rmulas y gr√°ficos
    """
    print("\n" + "="*70)
    print("PROBLEMA 11: CONVEXIDAD Y FALTA DE √ìPTIMOS LOCALES")
    print("="*70)
    
    # Explicaci√≥n te√≥rica
    explicacion = """
    ¬øPOR QU√â LA REGRESI√ìN LINEAL NO TIENE √ìPTIMOS LOCALES?
    
    RAZ√ìN PRINCIPAL: La funci√≥n de p√©rdida J(Œ∏) es CONVEXA
    
    1. FUNCI√ìN DE P√âRDIDA:
       J(Œ∏) = (1/(2m)) * Œ£(hŒ∏(x‚Å±) - y‚Å±)¬≤
            = (1/(2m)) * (XŒ∏ - y)·µÄ(XŒ∏ - y)
    
    2. MATRIZ HESSIANA (segundas derivadas):
       ‚àá¬≤J(Œ∏) = (1/m) * X·µÄX
    
    3. PROPIEDAD CLAVE: X·µÄX es SEMIDEFINIDA POSITIVA
       - Todos sus valores propios son ‚â• 0
       - Por lo tanto, ‚àá¬≤J(Œ∏) es semidefinida positiva
       - Esto implica que J(Œ∏) es CONVEXA
    
    4. CONSECUENCIAS DE LA CONVEXIDAD:
       - Solo existe UN M√çNIMO GLOBAL
       - No hay m√≠nimos locales
       - El descenso de gradiente SIEMPRE converge al √≥ptimo
       - Independientemente del punto inicial
    
    COMPARACI√ìN CON MODELOS NO CONVEXOS (ej. redes neuronales):
       - Pueden tener m√∫ltiples m√≠nimos locales
       - El gradiente puede quedar atrapado
       - La convergencia al √≥ptimo global no est√° garantizada
    """
    print(explicacion)
    
    # Demostraci√≥n visual con gr√°ficos
    print("\nGenerando demostraci√≥n visual de la convexidad...")
    
    # Crear datos de ejemplo simples (2 par√°metros)
    np.random.seed(42)
    X_simple = 2 * np.random.rand(50, 1)
    y_simple = 3 + 4 * X_simple + np.random.randn(50, 1) * 0.5
    
    # A√±adir bias term
    X_with_bias = np.c_[np.ones((50, 1)), X_simple]
    
    # Crear grid de par√°metros para visualizaci√≥n
    theta0_vals = np.linspace(0, 6, 50)  # Œ∏‚ÇÄ (bias)
    theta1_vals = np.linspace(0, 8, 50)  # Œ∏‚ÇÅ (pendiente)
    Theta0, Theta1 = np.meshgrid(theta0_vals, theta1_vals)
    
    # Calcular funci√≥n de p√©rdida para cada combinaci√≥n de par√°metros
    J_vals = np.zeros_like(Theta0)
    
    for i in range(len(theta0_vals)):
        for j in range(len(theta1_vals)):
            theta = np.array([Theta0[i, j], Theta1[i, j]])
            predictions = X_with_bias @ theta
            J_vals[i, j] = np.mean((predictions - y_simple.flatten())**2) / 2
    
    # Gr√°fico 1: Superficie 3D de la funci√≥n de p√©rdida
    fig = plt.figure(figsize=(15, 5))
    
    # Subplot 1: Superficie 3D
    ax1 = fig.add_subplot(131, projection='3d')
    surf = ax1.plot_surface(Theta0, Theta1, J_vals, cmap='viridis', 
                           alpha=0.8, linewidth=0, antialiased=True)
    ax1.set_xlabel('Œ∏‚ÇÄ (Bias)')
    ax1.set_ylabel('Œ∏‚ÇÅ (Pendiente)')
    ax1.set_zlabel('J(Œ∏)')
    ax1.set_title('Superficie Convexa de la\nFunci√≥n de P√©rdida')
    
    # Marcar el m√≠nimo global (soluci√≥n anal√≠tica)
    theta_optimal = np.linalg.inv(X_with_bias.T @ X_with_bias) @ X_with_bias.T @ y_simple
    min_J = np.mean((X_with_bias @ theta_optimal - y_simple.flatten())**2) / 2
    ax1.scatter(theta_optimal[0], theta_optimal[1], min_J, 
               color='red', s=100, label='M√≠nimo Global')
    
    # Subplot 2: Curvas de nivel
    ax2 = fig.add_subplot(132)
    contour = ax2.contour(Theta0, Theta1, J_vals, levels=20)
    ax2.clabel(contour, inline=1, fontsize=8)
    ax2.set_xlabel('Œ∏‚ÇÄ (Bias)')
    ax2.set_ylabel('Œ∏‚ÇÅ (Pendiente)')
    ax2.set_title('Curvas de Nivel - Forma de "Bowl"\n(Indica Convexidad)')
    ax2.grid(True, alpha=0.3)
    
    # Marcar el m√≠nimo en las curvas de nivel
    ax2.scatter(theta_optimal[0], theta_optimal[1], color='red', s=100, 
               label='M√≠nimo Global')
    ax2.legend()
    
    # Subplot 3: Demostraci√≥n de m√∫ltiples puntos iniciales
    ax3 = fig.add_subplot(133)
    
    # Puntos iniciales diferentes
    initial_points = [
        np.array([1.0, 2.0]),   # Punto 1
        np.array([5.0, 1.0]),   # Punto 2  
        np.array([2.0, 7.0]),   # Punto 3
        np.array([4.5, 6.0])    # Punto 4
    ]
    
    colors = ['blue', 'green', 'orange', 'purple']
    
    # Mostrar curvas de nivel
    contour = ax3.contour(Theta0, Theta1, J_vals, levels=15)
    
    for i, (point, color) in enumerate(zip(initial_points, colors)):
        # Simular trayectoria de descenso de gradiente (simplificado)
        current_point = point.copy()
        trajectory = [current_point.copy()]
        
        for step in range(10):  # 10 pasos de gradiente
            # Calcular gradiente (simplificado)
            theta_current = current_point
            predictions = X_with_bias @ theta_current
            error = predictions - y_simple.flatten()
            gradient = (X_with_bias.T @ error) / len(y_simple)
            
            # Actualizar
            current_point = current_point - 0.1 * gradient
            trajectory.append(current_point.copy())
        
        trajectory = np.array(trajectory)
        ax3.plot(trajectory[:, 0], trajectory[:, 1], 'o-', color=color, 
                linewidth=2, markersize=4, label=f'Punto inicial {i+1}')
        ax3.scatter(trajectory[0, 0], trajectory[0, 1], color=color, s=100, 
                   marker='s', edgecolors='black')
    
    ax3.scatter(theta_optimal[0], theta_optimal[1], color='red', s=150, 
               marker='*', label='M√≠nimo Global', edgecolors='black')
    ax3.set_xlabel('Œ∏‚ÇÄ (Bias)')
    ax3.set_ylabel('Œ∏‚ÇÅ (Pendiente)')
    ax3.set_title('M√∫ltiples Trayectorias de\nDescenso de Gradiente')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('graficos/demostracion_convexidad.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("Gr√°fico guardado: graficos/demostracion_convexidad.png")
    
    # Gr√°fico 4: Comparaci√≥n con funci√≥n no convexa
    fig = plt.figure(figsize=(12, 5))
    
    # Funci√≥n convexa (nuestra p√©rdida)
    ax1 = fig.add_subplot(121)
    contour1 = ax1.contour(Theta0, Theta1, J_vals, levels=15)
    ax1.set_xlabel('Œ∏‚ÇÄ')
    ax1.set_ylabel('Œ∏‚ÇÅ')
    ax1.set_title('FUNCI√ìN CONVEXA\n(Regresi√≥n Lineal)')
    ax1.grid(True, alpha=0.3)
    
    # Funci√≥n no convexa de ejemplo (Rastrigin)
    x = np.linspace(-5, 5, 100)
    y = np.linspace(-5, 5, 100)
    X, Y = np.meshgrid(x, y)
    Z = (X**2 - 10*np.cos(2*np.pi*X)) + (Y**2 - 10*np.cos(2*np.pi*Y)) + 20
    
    ax2 = fig.add_subplot(122)
    contour2 = ax2.contour(X, Y, Z, levels=15)
    ax2.set_xlabel('x')
    ax2.set_ylabel('y')
    ax2.set_title('FUNCI√ìN NO CONVEXA\n(M√∫ltiples m√≠nimos locales)')
    ax2.grid(True, alpha=0.3)
    
    # Marcar algunos m√≠nimos locales
    local_minima = [(-3.5, -3.5), (0, 0), (3.5, 3.5)]
    for min_point in local_minima:
        ax2.scatter(min_point[0], min_point[1], color='red', s=50)
    
    plt.tight_layout()
    plt.savefig('graficos/comparacion_convexidad.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("Gr√°fico guardado: graficos/comparacion_convexidad.png")
    
    # Explicaci√≥n adicional
    print("\n" + "-"*50)
    print("DEMOSTRACI√ìN COMPLETADA")
    print("-"*50)
    print("Los gr√°ficos muestran:")
    print("1. La superficie CONVEXA de la funci√≥n de p√©rdida")
    print("2. C√≥mo M√öLTIPLES puntos iniciales convergen al MISMO m√≠nimo")
    print("3. Comparaci√≥n con funciones NO CONVEXAS (m√∫ltiples m√≠nimos)")
    print("4. Esto explica por qu√© el aprendizaje continuo SIEMPRE encuentra el √≥ptimo")


# =============================================================================
# Funci√≥n para gr√°ficos de comparaci√≥n de predicciones
# =============================================================================
def crear_graficos_comparacion(y_test, y_pred_scratch, y_pred_sklearn, save_dir="graficos"):
    """Crea todos los gr√°ficos de comparaci√≥n de predicciones"""
    
    # Calcular diferencias
    differences = np.abs(y_pred_scratch - y_pred_sklearn)
    
    # 1. Gr√°fico: Predicciones vs Valores Reales
    plt.figure(figsize=(15, 5))
    
    # Subplot 1: Prediction vs Actual
    plt.subplot(1, 3, 1)
    plt.scatter(y_test, y_pred_scratch, alpha=0.5, label='Scratch', color='blue', s=20)
    plt.scatter(y_test, y_pred_sklearn, alpha=0.5, label='Sklearn', color='red', s=20)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', alpha=0.8, linewidth=2)
    plt.xlabel('Valor Real')
    plt.ylabel('Predicci√≥n')
    plt.title('Predicciones vs Valores Reales\n(Scratch vs Sklearn)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Subplot 2: Distribution of Prediction Differences
    plt.subplot(1, 3, 2)
    plt.hist(differences, bins=30, alpha=0.7, color='green', edgecolor='black')
    plt.xlabel('Diferencia entre Predicciones')
    plt.ylabel('Frecuencia')
    plt.title('Distribuci√≥n de Diferencias\nentre Scratch y Sklearn')
    plt.grid(True, alpha=0.3)
    
    # Subplot 3: Comparaci√≥n directa de predicciones
    plt.subplot(1, 3, 3)
    plt.scatter(y_pred_scratch, y_pred_sklearn, alpha=0.5, color='purple')
    plt.plot([y_pred_scratch.min(), y_pred_scratch.max()], 
             [y_pred_scratch.min(), y_pred_scratch.max()], 'k--', alpha=0.8)
    plt.xlabel('Predicciones Scratch')
    plt.ylabel('Predicciones Sklearn')
    plt.title('Scratch vs Sklearn\nComparaci√≥n Directa')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f"{save_dir}/comparacion_predicciones.png")
    plt.close()
    print(f"Gr√°fico guardado: {save_dir}/comparacion_predicciones.png")
    
    # 2. Gr√°fico: Errores de predicci√≥n
    plt.figure(figsize=(12, 5))
    
    errors_scratch = np.abs(y_test - y_pred_scratch)
    errors_sklearn = np.abs(y_test - y_pred_sklearn)
    
    plt.subplot(1, 2, 1)
    plt.hist(errors_scratch, bins=30, alpha=0.7, label='Scratch', color='blue')
    plt.hist(errors_sklearn, bins=30, alpha=0.7, label='Sklearn', color='red')
    plt.xlabel('Error Absoluto')
    plt.ylabel('Frecuencia')
    plt.title('Distribuci√≥n de Errores Absolutos')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.boxplot([errors_scratch, errors_sklearn], labels=['Scratch', 'Sklearn'])
    plt.ylabel('Error Absoluto')
    plt.title('Comparaci√≥n de Errores (Boxplot)')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f"{save_dir}/distribucion_errores.png")
    plt.close()
    print(f"Gr√°fico guardado: {save_dir}/distribucion_errores.png")

# =============================================================================
# Datos de prueba
# =============================================================================
def cargar_datos_habitacionales():
    """Crea datos de ejemplo para precios de viviendas"""
    np.random.seed(42)
    n_samples = 1000
    
    X = np.column_stack([
        np.random.normal(150, 50, n_samples),  # Tama√±o (m¬≤)
        np.random.normal(3, 1, n_samples),     # Habitaciones
        np.random.normal(2, 0.5, n_samples),   # Ba√±os
        np.random.normal(20, 5, n_samples),    # Antig√ºedad
        np.random.normal(10, 3, n_samples)     # Distancia al centro
    ])
    
    true_coef = np.array([1000, 50000, 30000, -10000, -20000])
    y = X @ true_coef + np.random.normal(0, 50000, n_samples)
    
    return X, y

# =============================================================================
# FUNCI√ìN PRINCIPAL - EJECUTA TODOS LOS PROBLEMAS
# =============================================================================
def main():
    """Funci√≥n principal que ejecuta todos los problemas del 1 al 11"""
    print("REGERSI√ìN LINEAL DESDE CERO - PROBLEMAS 1 AL 11")
    print("="*70)
    
    # Crear carpeta para gr√°ficos
    os.makedirs('graficos', exist_ok=True)
    
    # Cargar datos
    print("\n1. Cargando datos de precios de viviendas...")
    X, y = cargar_datos_habitacionales()
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Estandarizar
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print(f"   Datos de entrenamiento: {X_train_scaled.shape}")
    print(f"   Datos de prueba: {X_test_scaled.shape}")
    
    # [PROBLEMA 6] Entrenar y comparar modelos
    print("\n2. Entrenando modelo Scratch...")
    scratch_model = ScratchLinearRegression(num_iter=1000, lr=0.01, verbose=True)
    scratch_model.fit(X_train_scaled, y_train, X_test_scaled, y_test)
    
    # Predicciones
    y_pred_scratch = scratch_model.predict(X_test_scaled)
    mse_scratch = MSE(y_pred_scratch, y_test)
    
    # Comparaci√≥n con Scikit-learn
    print("\n3. Entrenando modelo Scikit-learn...")
    sklearn_model = LinearRegression()
    sklearn_model.fit(X_train_scaled, y_train)
    y_pred_sklearn = sklearn_model.predict(X_test_scaled)
    mse_sklearn = mean_squared_error(y_test, y_pred_sklearn)
    
    print(f"\n4. Comparaci√≥n de resultados:")
    print(f"   MSE Scratch: {mse_scratch:.6f}")
    print(f"   MSE Sklearn: {mse_sklearn:.6f}")
    print(f"   Diferencia: {abs(mse_scratch - mse_sklearn):.6f}")
    
    # [PROBLEMA 7] Curva de aprendizaje
    print("\n5. Generando curva de aprendizaje...")
    plot_learning_curve(scratch_model.loss, scratch_model.val_loss,
                       "Curva de Aprendizaje - Regresi√≥n Lineal",
                       "graficos/curva_aprendizaje.png")
    
    # [PROBLEMA 6] Gr√°ficos de comparaci√≥n de predicciones
    print("\n6. Generando gr√°ficos de comparaci√≥n...")
    crear_graficos_comparacion(y_test, y_pred_scratch, y_pred_sklearn)
    
    # [PROBLEMA 8] An√°lisis del bias term
    print("\n7. Analizando efecto del bias term...")
    
    # Con bias term
    model_with_bias = ScratchLinearRegression(num_iter=500, lr=0.01, no_bias=False, verbose=False)
    model_with_bias.fit(X_train_scaled, y_train, X_test_scaled, y_test)
    y_pred_with_bias = model_with_bias.predict(X_test_scaled)
    mse_with_bias = MSE(y_pred_with_bias, y_test)
    
    # Sin bias term
    model_no_bias = ScratchLinearRegression(num_iter=500, lr=0.01, no_bias=True, verbose=False)
    model_no_bias.fit(X_train_scaled, y_train, X_test_scaled, y_test)
    y_pred_no_bias = model_no_bias.predict(X_test_scaled)
    mse_no_bias = MSE(y_pred_no_bias, y_test)
    
    print(f"   MSE con bias: {mse_with_bias:.6f}")
    print(f"   MSE sin bias: {mse_no_bias:.6f}")
    print(f"   Mejora: {((mse_no_bias - mse_with_bias)/mse_no_bias*100):.2f}%")
    
    # Gr√°fico bias term
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(model_with_bias.loss, label='Con bias')
    plt.title('Con Bias Term')
    plt.legend()
    plt.grid(True)
    
    plt.subplot(1, 2, 2)
    plt.plot(model_no_bias.loss, label='Sin bias')
    plt.title('Sin Bias Term')
    plt.legend()
    plt.grid(True)
    
    plt.savefig('graficos/comparacion_bias.png')
    plt.close()
    print("   Gr√°fico guardado: graficos/comparacion_bias.png")
    
    # [PROBLEMA 9] Caracter√≠sticas polin√≥micas
    print("\n8. Probando caracter√≠sticas polin√≥micas...")
    degrees = [1, 2]
    
    for degree in degrees:
        try:
            X_train_poly = create_polynomial_features(X_train_scaled, degree)
            X_test_poly = create_polynomial_features(X_test_scaled, degree)
            
            model_poly = ScratchLinearRegression(num_iter=800, lr=0.001, verbose=False)
            model_poly.fit(X_train_poly, y_train, X_test_poly, y_test)
            
            y_pred_poly = model_poly.predict(X_test_poly)
            mse_poly = MSE(y_pred_poly, y_test)
            
            print(f"   Grado {degree}: MSE = {mse_poly:.6f}")
            
        except Exception as e:
            print(f"   Error con grado {degree}: {e}")
    
    # [PROBLEMA 10] Explicaci√≥n derivaci√≥n
    derivacion_formula_actualizacion()
    
    # [PROBLEMA 11] Explicaci√≥n convexidad
    demostrar_convexidad()
    
    # Resumen final
    print("\n" + "="*70)
    print("RESUMEN EJECUCI√ìN - PROBLEMAS 1 AL 11")
    print("="*70)
    print("‚úÖ Problema 1: Funci√≥n de suposici√≥n")
    print("‚úÖ Problema 2: Descenso de gradiente") 
    print("‚úÖ Problema 3: Funci√≥n de predicci√≥n")
    print("‚úÖ Problema 4: Funci√≥n MSE")
    print("‚úÖ Problema 5: Funci√≥n objetivo")
    print("‚úÖ Problema 6: Entrenamiento y validaci√≥n")
    print("‚úÖ Problema 7: Curvas de aprendizaje")
    print("‚úÖ Problema 8: An√°lisis de bias term")
    print("‚úÖ Problema 9: Caracter√≠sticas polin√≥micas")
    print("‚úÖ Problema 10: Derivaci√≥n matem√°tica")
    print("‚úÖ Problema 11: Convexidad con demostraci√≥n gr√°fica")
    print("="*70)
    print("\nGR√ÅFICOS GENERADOS:")
    print("üìä graficos/curva_aprendizaje.png")
    print("üìä graficos/comparacion_predicciones.png")
    print("üìä graficos/distribucion_errores.png") 
    print("üìä graficos/comparacion_bias.png")
    print("üìä graficos/demostracion_convexidad.png")  # NUEVO
    print("üìä graficos/comparacion_convexidad.png") 
    print("\nEjecuci√≥n completada exitosamente! ")

if __name__ == "__main__":
    main()